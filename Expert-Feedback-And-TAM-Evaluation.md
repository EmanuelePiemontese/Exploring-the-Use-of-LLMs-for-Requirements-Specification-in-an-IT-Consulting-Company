### Expert Feedback and TAM Evaluation:
<p>
To assess the usability of AI-generated documentation, a structured interview was conducted with a functional analyst who directly worked on the project. The interview was structured into several key evaluation phases. First, the analyst assessed a randomly selected user story to evaluate its coherence, clarity, conciseness, and completeness. Additionally, the analyst compared the generated user story with what would have been produced manually. Second, the evaluation focused on the Epic FDS, analyzing the modelâ€™s ability to generate detailed and coherent technical documentation, with particular emphasis on the clarity and adaptability of the epic. Third, the analyst examined the FDS of the previously assessed user story, focusing on the technical specifications and level of detail provided. Fourth, a comparative analysis was conducted between manually created FDS documents and those generated by the model, using 19 initial user stories as a reference. This phase aimed to determine whether the model-generated documents could be distinguished from those produced by human analysts. The fifth phase involved an overall evaluation of LLMs in the requirements engineering process, exploring potential time savings and identifying the most beneficial stages of the requirements lifecycle for model utilization. In the final phase, the TAM, for assessing technology adoption, was applied. This model evaluates two primary factors. <strong>Perceived</strong> Usefulness, which measures the extent to which a technology is believed to enhance work performance, and <strong>Perceived Ease of Use</strong>, which assesses the effort required to use the technology effectively. The analyst was asked Likert-scale-based questions (ranging from 1 meaning strongly disagreed to 5 meaning completely agreed) to assess the perceived benefits of using LLMs in RE. The evaluation criteria included efficiency (whether the model allows analysts to complete tasks more quickly), performance improvement (whether the model enhances the quality of generated documents), and overall productivity (how the model facilitates requirements management). The following section presents the results obtained from the interview. The structured expert evaluation results are summarized in Tables \ref{tab:user_story_evaluation}-\ref{tab:tam_evaluation}. The user story evaluation, presented in Table \ref{tab:user_story_evaluation}, shows that AI-generated content was well-structured and clear but lacked critical details. GPT4 achieved the highest readability score, while GPT3 had more redundant information. Analysts noted that AI-generated stories were coherent but often required additional refinement to match human-created ones.
</p>

<table style="width:100%; border-collapse: collapse;" border="1">
  <caption><strong>User story evaluation results</strong></caption>
  <thead>
    <tr>
      <th style="width:20%; text-align: center;"><strong>Criteria</strong></th>
      <th style="width:15%; text-align: center;"><strong>Score</strong></th>
      <th style="width:50%; text-align: left;"><strong>Notes</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">Coherence and Structure</td>
      <td style="text-align: center;">3/5</td>
      <td>The structure was clear but lacked detail.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Clarity</td>
      <td style="text-align: center;">4/5</td>
      <td>The story was easy to follow and logically consistent.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Conciseness</td>
      <td style="text-align: center;">4/5</td>
      <td>Information was to the point without unnecessary elaboration.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Completeness</td>
      <td style="text-align: center;">3/5</td>
      <td>Some crucial information was missing.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Adaptability to Future Changes</td>
      <td style="text-align: center;">3/5</td>
      <td>The user story was rigid and might require modifications later.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Comparison to Human-Written Stories</td>
      <td style="text-align: center;">5/5</td>
      <td>Human analysts provided more detailed and nuanced content.</td>
    </tr>
  </tbody>
</table>

<p align="justify">
The Epic FDS evaluation, detailed in Table \ref{tab:epic_fds_evaluation}, indicates that AI models struggled to produce deep functional documentation. While the generated FDS followed structured templates, the content often resembled a high-level checklist rather than a comprehensive document, requiring significant analyst input.
</p>

<table style="width:100%; border-collapse: collapse;" border="1">
  <caption><strong>Epic FDS Evaluation</strong></caption>
  <thead>
    <tr>
      <th style="width:30%; text-align: center;"><strong>Criteria</strong></th>
      <th style="width:15%; text-align: center;"><strong>Score</strong></th>
      <th style="width:40%; text-align: left;"><strong>Notes</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">Coherence and Structure</td>
      <td style="text-align: center;">3/5</td>
      <td>Logical structure but too generic.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Clarity and Completeness</td>
      <td style="text-align: center;">2/5</td>
      <td>Content resembled a checklist rather than a deep functional document.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Comparison to Human-Written FDS</td>
      <td style="text-align: center;">3/5</td>
      <td>Human analysts added more technical depth and domain-specific language.</td>
    </tr>
  </tbody>
</table>

<p align="justify">
In the FDS document evaluation (Table \ref{tab:user_story_fds_evaluation}), AI-generated FDS documents lacked technical depth and precision, making them difficult for direct use in software development. The analyst found that human-written FDS documents were far more contextually accurate and detailed.
</p>

<table style="width:100%; border-collapse: collapse;" border="1">
  <caption><strong>User Story FDS Evaluation</strong></caption>
  <thead>
    <tr>
      <th style="width:30%; text-align: center;"><strong>Criteria</strong></th>
      <th style="width:15%; text-align: center;"><strong>Score</strong></th>
      <th style="width:40%; text-align: left;"><strong>Notes</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">Technical Accuracy</td>
      <td style="text-align: center;">2/5</td>
      <td>Contained general information but lacked precise implementation details.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Detail Level and Usability</td>
      <td style="text-align: center;">2/5</td>
      <td>Developers would struggle to use the document as-is.</td>
    </tr>
  </tbody>
</table>

<p align="justify">
Table \ref{tab:human_vs_ai_comparison} presents the comparison of human vs. AI-generated documents, where analysts attempted to distinguish AI-created content. The results indicate that AI-generated documentation could be identified due to its lack of deep domain-specific language and nuanced explanations.
</p>

<table style="width:100%; border-collapse: collapse;" border="1">
  <caption><strong>Human vs. AI Document Comparison</strong></caption>
  <thead>
    <tr>
      <th style="width:30%; text-align: center;"><strong>Section Evaluated</strong></th>
      <th style="width:15%; text-align: center;"><strong>AI-Generated Score</strong></th>
      <th style="width:40%; text-align: left;"><strong>Notes</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">Executive Summary</td>
      <td style="text-align: center;">2/2</td>
      <td>AI maintained basic structure but lacked depth.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Overview/Background</td>
      <td style="text-align: center;">1/2</td>
      <td>AI document was well-structured but imprecise.</td>
    </tr>
    <tr>
      <td style="text-align: center;">Assumptions and Dependencies</td>
      <td style="text-align: center;">2/2</td>
      <td>AI content followed templates but needed manual refinement.</td>
    </tr>
  </tbody>
</table>


<p>
Finally, Table \ref{tab:tam_evaluation} summarizes the TAM evaluation results, revealing that while LLMs improved efficiency and provided useful support in Requirements Engineering, they still required manual intervention for refinement and adaptation. AI-assisted documentation was considered useful in the initial drafting phase but not reliable for standalone use.
</p>

<table style="width:100%; border-collapse: collapse;" border="1">
  <caption><strong>TAM Evaluation Results</strong></caption>
  <thead>
    <tr>
      <th style="width:30%; text-align: center;"><strong>Evaluation Criteria</strong></th>
      <th style="width:15%; text-align: center;"><strong>Score</strong></th>
      <th style="width:40%; text-align: left;"><strong>Notes</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">LLMs help complete tasks faster</td>
      <td style="text-align: center;">3/5</td>
      <td>AI was useful in initial drafting but required refinement.</td>
    </tr>
    <tr>
      <td style="text-align: center;">LLMs improve document quality</td>
      <td style="text-align: center;">3/5</td>
      <td>Quality was inconsistent, requiring post-processing.</td>
    </tr>
    <tr>
      <td style="text-align: center;">LLMs increase productivity</td>
      <td style="text-align: center;">4/5</td>
      <td>Helped streamline documentation but not without oversight.</td>
    </tr>
    <tr>
      <td style="text-align: center;">LLMs provide useful support in RE</td>
      <td style="text-align: center;">4/5</td>
      <td>Effective for assisting in user story drafting.</td>
    </tr>
    <tr>
      <td style="text-align: center;">LLMs are useful in daily work</td>
      <td style="text-align: center;">4/5</td>
      <td>Analysts found AI-generated drafts beneficial.</td>
    </tr>
  </tbody>
</table>


<p>
The interview revealed that LLMs are effective in generating coherent and well-structured documents, such as user stories and epics, but they lack detail and adaptability. While user stories were clear and concise, they were incomplete, and epics were simplistic outlines lacking full functionality coverage. The technical specifications generated were also not detailed enough for effective development guidance. Despite these limitations, LLMs provide a time-saving benefit of 15 to 30 minutes per 4 hours of work, particularly in creating epics and summary documents. Additionally, TAM assessment showed a moderately positive perception of LLMs' usefulness, especially for repetitive tasks and standardized documentation. Overall, LLMs can enhance productivity in the early phases of documentation and requirements definition but cannot replace human intervention. Expert analysts are still needed to ensure technical accuracy, completeness, and adaptability of documents, as LLM-generated drafts require significant revision and integration for complex projects.
</p>